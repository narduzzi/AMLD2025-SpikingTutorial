{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/narduzzi/AMLD2025-SpikingTutorial/blob/master/AMLD2025_TinyML_Workshop_NeuromorphicTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Copyright (c) 2025 Simon Narduzzi\n",
        "# Licensed under the MIT License (https://opensource.org/licenses/MIT)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ECtyAUx962iU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My74cBWv5uiV"
      },
      "source": [
        "# AMLD 2025 - Neuromorphic Computing Tutorial\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Google Colab notebook provides a tutorial on neuromorphic computing using the Sinabs and Tonic libraries. It covers the following parts:\n",
        "\n",
        "- Introduction to Spiking Neural Networks (SNNs)\n",
        "- Event-based data processing\n",
        "- SNN models (IAF, LIF, ALIF)\n",
        "- Backpropagation and surrogate gradients\n",
        "- Training and evaluation of SNNs\n",
        "- Comparison of computational cost and accuracy.\n",
        "\n",
        "The notebook also includes hands-on examples for users to practice. While GPU use is recommended, it is not mandatory to rely on GPU to run the code contained in this notebook."
      ],
      "metadata": {
        "id": "SE8BEzqp556r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8KVR5y-mS3g",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# First, install the requirements\n",
        "!pip install matplotlib seaborn numpy sinabs torchvision scikit-learn\n",
        "!pip install git+https://github.com/neuromorphs/tonic.git@775e1ce5e0ffaeb42b43cc54cfed3ffb490809e7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGk2TM_2mZZC"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import tonic\n",
        "import sinabs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# set style for the notebook\n",
        "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
        "sns.set_theme(style=\"ticks\", rc=custom_params)\n",
        "\n",
        "colors = [\"#00a1e5\", \"#ffcc33\", \"#bfd100\", \"#d61e5c\", \"#878787\", \"#003264\", \"#3fbac1\"]\n",
        "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfVZqLYb9xFd"
      },
      "source": [
        "# Part 1: Introduction to Spiking Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZaRD7ll3mvE"
      },
      "source": [
        "## Event-based signal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Event-based signals are fundamentally different from traditional continuous signals. Instead of representing information as a continuous stream of values, event-based signals, also known as spike trains, represent information as a binary sequence (0 or 1) of discrete events or spikes that occur at specific times.\n",
        "\n",
        "Event-based signals are usually sparse (>90% sparsity) time-series. The event trigger computation downstream only when they are received by the neurons.\n"
      ],
      "metadata": {
        "id": "fbfHZhYwqywF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dniPlvYZql0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnnqDtvf3pkk"
      },
      "outputs": [],
      "source": [
        "# generate random spikes\n",
        "np.random.seed(30) # for repeatability\n",
        "n_neurons = 10\n",
        "t_max = 200 # ms\n",
        "stimuli = np.random.random((n_neurons, t_max)) > 0.9\n",
        "\n",
        "neuron_idx, t_idx = np.where(stimuli)\n",
        "# plot the sample\n",
        "plot_trace = False\n",
        "neuron_of_interest = 3\n",
        "\n",
        "fig, axes = plt.subplots(1,2 if plot_trace else 1)\n",
        "axes = [axes] if not plot_trace else axes\n",
        "\n",
        "fig.set_figwidth(15)\n",
        "fig.set_figheight(5)\n",
        "axes[0].scatter(t_idx, neuron_idx, s=10, color=\"k\")\n",
        "\n",
        "idx_of_neuron_of_interest = np.where(neuron_idx == neuron_of_interest)\n",
        "\n",
        "axes[0].scatter(t_idx[idx_of_neuron_of_interest], neuron_idx[idx_of_neuron_of_interest], s=10, color=colors[0], label=\"Neuron of interest: #{}\".format(neuron_of_interest))\n",
        "axes[0].set_title(\"Random spike trains\")\n",
        "axes[0].set_xlabel(\"Time (ms)\")\n",
        "axes[0].set_ylabel(\"Neuron idx\")\n",
        "\n",
        "# plot one single spike train\n",
        "if plot_trace:\n",
        "  axes[1].plot(stimuli[neuron_of_interest], color=\"b\")\n",
        "  axes[1].set_title(\"Values of neuron #{}\".format(neuron_of_interest))\n",
        "  axes[1].set_xlabel(\"Time (ms)\")\n",
        "  axes[1].set_ylabel(\"Value\")\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random spike trains are generated. In the next section, we will focus on a single neuron spike train that will be fed into different neuron types."
      ],
      "metadata": {
        "id": "0f6jpclnrRq8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXz7myh-yCoV"
      },
      "source": [
        "## Spiking Neuron Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tG2oBplb26Ok"
      },
      "source": [
        "In this section, we will explore different models of feed-forward spiking neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q04hEGzDz_8u"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<svg width=\"600\" height=\"200\">\n",
        "    <!-- Synapse -->\n",
        "    <line x1=\"50\" y1=\"100\" x2=\"250\" y2=\"100\" stroke=\"#2c3e50\" stroke-width=\"6\"/>\n",
        "    <!-- Soma -->\n",
        "    <circle cx=\"250\" cy=\"100\" r=\"30\" stroke=\"#2c3e50\" stroke-width=\"6\" fill=\"#888888\"/>\n",
        "    <!-- Axon -->\n",
        "    <line x1=\"280\" y1=\"100\" x2=\"500\" y2=\"100\" stroke=\"#2c3e50\" stroke-width=\"6\"/>\n",
        "    <polygon points=\"500,90 520,100 500,110\" fill=\"#2c3e50\"/>\n",
        "\n",
        "    <!-- Synaptic Spikes -->\n",
        "    <line x1=\"70\" y1=\"70\" x2=\"70\" y2=\"90\" stroke=\"#00a1e5\" stroke-width=\"4\"/>\n",
        "    <line x1=\"90\" y1=\"70\" x2=\"90\" y2=\"90\" stroke=\"#00a1e5\" stroke-width=\"4\"/>\n",
        "    <line x1=\"110\" y1=\"70\" x2=\"110\" y2=\"90\" stroke=\"#00a1e5\" stroke-width=\"4\"/>\n",
        "\n",
        "    <!-- Axonal Spikes -->\n",
        "    <line x1=\"420\" y1=\"70\" x2=\"420\" y2=\"90\" stroke=\"#d61e5c\" stroke-width=\"4\"/>\n",
        "    <line x1=\"440\" y1=\"70\" x2=\"440\" y2=\"90\" stroke=\"#d61e5c\" stroke-width=\"4\"/>\n",
        "\n",
        "    <!-- Synaptic Triangle -->\n",
        "    <polygon points=\"160,65 180,85 150,100\" fill=\"#00a1e5\"/>\n",
        "\n",
        "    <!-- Soma Triangle -->\n",
        "    <polygon points=\"250,40 270,60 240,75\" fill=\"#ffcc33\"/>\n",
        "\n",
        "    <!-- Labels -->\n",
        "\n",
        "    <text x=\"50\" y=\"130\" font-size=\"16\" font-family=\"Arial\">Synapse</text>\n",
        "    <text x=\"230\" y=\"105\" font-size=\"16\" font-family=\"Arial\">Soma</text>\n",
        "    <text x=\"480\" y=\"130\" font-size=\"16\" font-family=\"Arial\">Axon</text>\n",
        "</svg>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_**Generic spiking neuron model**_\n",
        "\n",
        "The spiking neuron abstractions we will consider here have three main parts:\n",
        "\n",
        "- **Synapse**: the synapse weights and filters the incoming spikes, leading to an input current that is fed to the soma. The value $\\tau_{syn}$ describes the time-scale under which it is filtered.\n",
        "\n",
        "- **Soma**: the soma is the core of the neuron. The input is accumulated in the membrane of the soma. The membrane has a threshold value and a reset value that trigger the spike dynamics. The membrane can have a leak on a timescale defined by $\\tau_{mem}$. It can also include different mechanisms, such as adaptation.\n",
        "\n",
        "- **Axon**: the axon is responsible of forwarding the spikes to the next neuron's synapses.\n",
        "\n",
        "$ $\n",
        "\n",
        "\n",
        "_**IAF, LIF and ALIF**_\n",
        "\n",
        "Here, we explore three different types of neurons:\n",
        "- **Integrate-and-Fire (IAF)**: Simple neuron, with only synaptic leak. The membrane has no leak and only accumulates the input.\n",
        "- **Leaky Integrate-and-Fire (LIF)**: has a membrane leak term\n",
        "- **Adaptive LIF (ALIF)**: has a threshold adaptation mechanism, which limits the firing rate by increasing the threshold after each spike. In case of bursts of inputs, the output spike frequency will be reduced (slowed-down firing).\n",
        "\n",
        "Please refer to the [documentation of Sinabs](https://sinabs.readthedocs.io/en/v2.0.2/api/layers.html) for details about the formulas and implementations of these neurons."
      ],
      "metadata": {
        "id": "qSL372j-rhJV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZiVMsXYmYnO"
      },
      "outputs": [],
      "source": [
        "# Define IAF, LIF and AdLIF layers. The 'record_states' argument is passed to store the membrane potential.\n",
        "# The time constants (taus) are choose empirically for the purpose of this demonstration.\n",
        "iaf = sinabs.layers.IAF(tau_syn=10., min_v_mem=0, spike_fn=sinabs.activation.SingleSpike,  record_states=True)\n",
        "lif = sinabs.layers.LIF(tau_mem=20., tau_syn=10.,  min_v_mem=0, spike_fn=sinabs.activation.SingleSpike,  record_states=True, norm_input=False)\n",
        "adlif = sinabs.layers.ALIF(tau_mem=20., tau_syn=10., tau_adapt=30., spike_fn=sinabs.activation.SingleSpike,  record_states=True, norm_input=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYWadCuN8rVu"
      },
      "outputs": [],
      "source": [
        "# Select the stimuli (spike train) of the neuron of interest\n",
        "input_stimuli = torch.from_numpy(stimuli[neuron_of_interest])\n",
        "\n",
        "input_x = input_stimuli.unsqueeze(0).unsqueeze(-1)\n",
        "print(\"Input shape: {}\".format(input_x.shape))\n",
        "\n",
        "weight = 0.10 # arbitrary synaptic weight\n",
        "\n",
        "# perform inference on each neuron type\n",
        "output_iaf = iaf(input_x * weight)\n",
        "output_lif = lif(input_x * weight)\n",
        "output_adlif = adlif(input_x * weight)\n",
        "\n",
        "# print size (B,T,U): They are time-series of 1 sample, 200 timesteps, 1 neuron.\n",
        "print(\"IAF output: {}\".format(output_iaf.shape))\n",
        "print(\"LIF output: {}\".format(output_lif.shape))\n",
        "print(\"AdLIF output: {}\".format(output_adlif.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muEASNd6AD_w"
      },
      "outputs": [],
      "source": [
        "# Different neurons have different recording values\n",
        "iaf.recordings.keys(), lif.recordings.keys(), adlif.recordings.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4uUuJWcFXAz"
      },
      "outputs": [],
      "source": [
        "def plot_recordings(i_spike, o_spike, recordings, keys, title, axes=None, column=None, continuous=False):\n",
        "    \"\"\"\n",
        "    This function plots the recording of the neuron, bottom up.\n",
        "    The bottom plots the input stimuli. The plot in the internal dynamics.\n",
        "    The top plot is the output spikes.\n",
        "    \"\"\"\n",
        "    if axes is None:\n",
        "        fig, ax = plt.subplots(3,1)\n",
        "        fig.set_figwidth(15)\n",
        "        fig.set_figheight(5)\n",
        "\n",
        "    # get spikes indices\n",
        "    _, input_spike_t, _ = np.where(i_spike.detach().numpy())\n",
        "    ax = axes[2, column]\n",
        "    if continuous:\n",
        "        ax.plot(i_spike.detach().numpy()[0,:,0], label=\"v_mem\")\n",
        "    else:\n",
        "      ax.scatter(input_spike_t, np.ones(input_spike_t.shape), s=10, label=\"Input Spikes\")\n",
        "    ax.set_ylabel(\"Input\")\n",
        "    ax.set_yticks([])\n",
        "    ax.set_xlabel(\"Time (ms)\")\n",
        "\n",
        "    _, output_spike_t, _ = np.where(o_spike.detach().numpy())\n",
        "    ax = axes[0, column]\n",
        "    ax.set_ylabel(\"Output\")\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(title)\n",
        "    ax.scatter(output_spike_t, np.ones(output_spike_t.shape)*1.02, s=10, c=colors[3], label=\"Output Spikes\")\n",
        "\n",
        "    ax = axes[1, column]\n",
        "    for i,k in enumerate(keys):\n",
        "        ax.plot(recordings[k].detach().numpy()[0,:,0], label=k, color=colors[i])\n",
        "    ax.set_ylabel(\"Cell value\")\n",
        "    ax.legend()\n",
        "    sns.despine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXdpKJNuyEiQ"
      },
      "outputs": [],
      "source": [
        "# Plot the results\n",
        "fig, axes = plt.subplots(3,3, sharex=True)\n",
        "fig.set_figwidth(15)\n",
        "fig.set_figheight(6)\n",
        "\n",
        "plot_recordings(input_x, output_iaf, iaf.recordings, [\"i_syn\",\"v_mem\"], \"IAF\", axes, column=0)\n",
        "plot_recordings(input_x, output_lif, lif.recordings, [\"i_syn\",\"v_mem\"], \"LIF\", axes, column=1)\n",
        "plot_recordings(input_x, output_adlif, adlif.recordings, [\"i_syn\",\"v_mem\", \"spike_threshold\"], \"ALIF\", axes, column=2)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we observe that the IAF has the higher spike rate, as the membrane is never allowed to discharge.\n",
        "\n",
        "The LIF membrane discharges when the input is silent, slowly going down to the resting potential. However, when a new spike happens, the membranes potential goes up again, and can reaches the threshold.\n",
        "\n",
        "The ALIF neuron shows adaptation of the threshold, effectively leading to a lower firing rate at the input, compared to other neurons. However, the adaptation comes at the cost of additional operations happening in the neuron model.\n",
        "\n",
        "We can also apply a fixed current instead of binary spikes. This will lead to regular firing in all models."
      ],
      "metadata": {
        "id": "WJ6I9AmnwABU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dBF1KckL_Cu"
      },
      "outputs": [],
      "source": [
        "# Constant input current\n",
        "\n",
        "# perform inference\n",
        "# reshape to (batch, timesteps, dim)\n",
        "input_stimuli = torch.from_numpy(np.ones(200))\n",
        "\n",
        "input_x = input_stimuli.unsqueeze(0).unsqueeze(-1)\n",
        "print(\"Input shape: {}\".format(input_x.shape))\n",
        "\n",
        "weight = 0.008  # synaptic weight\n",
        "\n",
        "# reset membranes potentials\n",
        "iaf.reset_states()\n",
        "lif.reset_states()\n",
        "adlif.reset_states()\n",
        "\n",
        "output_iaf = iaf(input_x * weight)\n",
        "output_lif = lif(input_x * weight)\n",
        "output_adlif = adlif(input_x * weight)\n",
        "\n",
        "# Plot the results\n",
        "fig, axes = plt.subplots(3,3, sharex=True)\n",
        "fig.set_figwidth(15)\n",
        "fig.set_figheight(5)\n",
        "\n",
        "plot_recordings(input_x, output_iaf, iaf.recordings, [\"i_syn\",\"v_mem\"], \"IAF\", axes, column=0, continuous=True)\n",
        "plot_recordings(input_x, output_lif, lif.recordings, [\"i_syn\",\"v_mem\"], \"LIF\", axes, column=1, continuous=True)\n",
        "plot_recordings(input_x, output_adlif, adlif.recordings, [\"i_syn\",\"v_mem\", \"b\"], \"ALIF\", axes, column=2, continuous=True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbIts868H_iJ"
      },
      "source": [
        "## Backpropagation in Spiking Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRA0yaAqNqKY"
      },
      "source": [
        "When modeling spiking neural networks, The Dirac delta function represents an infinitely narrow impulse, often used to model an instantaneous event or spike.\n",
        "\n",
        "$$\n",
        "S(t)= \\sum_i \\delta(tâˆ’t_i)\n",
        "$$\n",
        "\n",
        "In computer simulation, instead of the infinite impulse of the Dirac function, we use the Heaviside function to represent the threshold crossing function, which output a value of $1$:\n",
        "\n",
        "$$\n",
        "\\mathrm{H}(x) = \\begin{cases}\n",
        "    0 & x < 0 \\\\\n",
        "    1 & x >= 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The reset mechanisms takes care of bringing the membrane potential back to the resting potential.\n",
        "\n",
        "However, the networks can not be trained directly on spikes, as the derivative of the Heaviside function is not differentiable. This makes the training of spiking networks with backpropagation impossible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA_i5PfjSCGu"
      },
      "source": [
        "**Surrogate Gradients come to the rescue**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPyCVl-xFn7D"
      },
      "source": [
        "Surrogate gradients are a technique used to overcome this challenge. They are differentiable approximations of the non-differentiable spike functions. By using surrogate gradients, we can effectively \"trick\" backpropagation into working with SNNs, by switching the function on the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNnRxTErITe8"
      },
      "outputs": [],
      "source": [
        "# Functions that can be used as forward or backward. In spiking networks, Heaviside is usually used for binary thresholding.\n",
        "def heaviside(x):\n",
        "  return np.heaviside(x, 0)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def fast_tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "# Derivatives (can be used as surrogates)\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def fast_tanh_derivative(x):\n",
        "  return 1 - np.tanh(x)**2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f0ivP82SzTs"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1,2, sharex=True, sharey=True)\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "\n",
        "axes[0].plot(x, heaviside(x), label=r\"$H(x)$\")\n",
        "axes[0].plot(x, sigmoid(x), \"--\", label=r\"$f(x)$\")\n",
        "axes[0].plot(x, sigmoid_derivative(x), label=r\"$\\frac{df}{dx}(x)$\")\n",
        "axes[0].set_xlabel(\"x\")\n",
        "axes[0].set_ylabel(\"Function value\")\n",
        "axes[0].set_title(\"Sigmoid\")\n",
        "axes[0].grid(True)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(x, heaviside(x), label=r\"$H(x)$\")\n",
        "axes[1].plot(x, fast_tanh(x), \"--\")\n",
        "axes[1].plot(x, fast_tanh_derivative(x))\n",
        "axes[1].set_xlabel(\"x\")\n",
        "axes[1].set_title(\"Fast Tanh\")\n",
        "axes[1].grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can choose a surrogate function for the backward pass. The backpropagation algorithm would behave as if the forward was using the original activation $f(x)$. The selection of the best surrogate gradient is a hot topic in neuromorphic research."
      ],
      "metadata": {
        "id": "ZDpn1QDgzhNL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVgLwIBbEZ9N"
      },
      "source": [
        "# Part 2: Event-based audio dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will explore the use of the Spiking Heidelberg Digits (SHD), an audio event-based dataset that classify recordings of digits in german and english (20 classes, 10 digits in german and 10 in english)."
      ],
      "metadata": {
        "id": "RijVWICXz417"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXTl15aKFdi-"
      },
      "source": [
        "## Data loading and exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the [Tonic](https://github.com/neuromorphs/tonic) library to load the dataset. The download is handled by the library, which also provides transformation functions to prepare the dataset for the use with spiking neural networks."
      ],
      "metadata": {
        "id": "LIbjEvgO0N7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhzlKMHjEkAR"
      },
      "outputs": [],
      "source": [
        "dataset_train = tonic.datasets.SHD(save_to= \"./data_shd\", train= True)\n",
        "dataset_test = tonic.datasets.SHD(save_to= \"./data_shd\", train= False)\n",
        "\n",
        "print(\"Training samples:\", len(dataset_train))\n",
        "print(\"Testing samples:\", len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzLEXbJaFaRH"
      },
      "outputs": [],
      "source": [
        "# plot one audio sample\n",
        "events0, label0 = dataset_train[0]\n",
        "events1, label1 = dataset_train[6]\n",
        "\n",
        "fig, axes = plt.subplots(1,2, sharey=True)\n",
        "axes[0].scatter(events0[\"t\"], events0[\"x\"], s=10)\n",
        "axes[0].set_title(\"Label: {}\".format(label0))\n",
        "\n",
        "axes[1].scatter(events1[\"t\"], events1[\"x\"], s=10)\n",
        "axes[1].set_title(\"Label: {}\".format(label1))\n",
        "\n",
        "axes[0].set_ylabel(\"Neuron idx (channel)\")\n",
        "axes[0].set_xlabel(\"Time (us)\")\n",
        "axes[1].set_xlabel(\"Time (us)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The samples of the dataset come as lists of events (x,t,p), as it reduces the memory footprint of the dataset. However, neural networks can not read lists. We therefore have to transform the events in a readable (frame-based) format.\n",
        "\n",
        "We first analyse the distribution of events, and trim the sequence to reduce the memory footprint."
      ],
      "metadata": {
        "id": "zYqKWu3p0j7K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONjCmD6SE0E4"
      },
      "outputs": [],
      "source": [
        "# Plot distribution of events\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "max_t = 0\n",
        "all_train_t = []\n",
        "for i in tqdm(range(len(dataset_train))):\n",
        "  t = dataset_train[i][0][\"t\"].tolist()\n",
        "  max_t = max(max_t, max(t))\n",
        "  all_train_t+=t\n",
        "\n",
        "all_test_t = []\n",
        "for i in tqdm(range(len(dataset_test))):\n",
        "  t = dataset_test[i][0][\"t\"].tolist()\n",
        "  max_t = max(max_t, max(t))\n",
        "  all_test_t+=t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XarI-Z4GE4ie"
      },
      "outputs": [],
      "source": [
        "hist_train, bin_edges_train = np.histogram(all_train_t, bins=100)\n",
        "hist_test, bin_edges_test = np.histogram(all_test_t, bins=100)\n",
        "# Compute the 99th percentile\n",
        "percentile = 99\n",
        "p_train = np.percentile(all_train_t, percentile)\n",
        "p_test = np.percentile(all_test_t, percentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D53GqtJ3E64l"
      },
      "outputs": [],
      "source": [
        "# Compute bin centers\n",
        "bin_centers_train = (bin_edges_train[:-1] + bin_edges_train[1:]) / 2\n",
        "bin_centers_test = (bin_edges_test[:-1] + bin_edges_test[1:]) / 2\n",
        "\n",
        "# Plot histogram\n",
        "fig, ax = plt.subplots(1,1)\n",
        "axes = [ax]\n",
        "# histogram\n",
        "axes[0].bar(bin_centers_train, hist_train, width=np.diff(bin_edges_train), edgecolor=None, alpha=0.7, label=\"train\")\n",
        "axes[0].bar(bin_centers_test, hist_test, width=np.diff(bin_edges_test), edgecolor=None, alpha=0.7, label=\"test\")\n",
        "axes[0].set_xlabel(\"Time (s)\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "axes[0].set_title(\"Histogram\")\n",
        "\n",
        "# plot percentile vline\n",
        "axes[0].axvline(p_train, color=colors[0], linestyle='--', label=\"{}th percentile (train)\".format(percentile))\n",
        "axes[0].axvline(p_test, color=colors[1], linestyle='--', label=\"{}th percentile (test)\".format(percentile))\n",
        "# plot max_t\n",
        "axes[0].axvline(max_t, color=colors[2], linestyle='--', label=r'$t_{max}$')\n",
        "\n",
        "axes[0].legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "99% of the events happen before 0.7s. The Maximum duration of the sequence is 1.4s. As networks learn using batch-representation, we trim the remaining part of the sequences which do not contain lots of information, effectively reducing the simulation time by 50%."
      ],
      "metadata": {
        "id": "767G-TrV1Fiz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iADBJ4-GTkm"
      },
      "source": [
        "## Preprocessing of audio events"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We load again the dataset, but this time we apply transforms to trim and transform the events to frames when accessing a sample.\n",
        "\n",
        "To further reduce the memory footprint and reduce the simulation time, we downscale the number of channels by aggregating the signal, and accumulate the spikes in frames of 10ms. This is called binning. Each frame (bin) contains the spike counts of neighboring neurons that have spiked during the window of 10ms."
      ],
      "metadata": {
        "id": "kp2vdsoW1hDD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4Yc4H6IGddB"
      },
      "outputs": [],
      "source": [
        "# create transforms\n",
        "sensor_size = tonic.datasets.SHD.sensor_size # 700x1\n",
        "downsample = 5\n",
        "new_sensor_size = (sensor_size[0] // downsample, 1, 1)\n",
        "\n",
        "max_duration = 0.7 * 1e6\n",
        "\n",
        "frame_dt = 1e-3 # 1ms\n",
        "\n",
        "\n",
        "crop_time_transform = tonic.transforms.CropTime(max=max_duration)\n",
        "\n",
        "to_frame = tonic.transforms.ToFrame(\n",
        "    sensor_size=sensor_size,\n",
        "    time_window=frame_dt*1e6,\n",
        ")\n",
        "\n",
        "def bin_frame(downsample_t, downsample_c):\n",
        "  \"\"\"Downsamples the frame by summing the values of the frames\"\"\"\n",
        "  def agg(frame):\n",
        "    \"\"\"Input of frame is T, W, C\"\"\"\n",
        "    reshaped_frame = frame.reshape((frame.shape[0]//downsample_t, downsample_t, frame.shape[1], frame.shape[2]//downsample_c, downsample_c))\n",
        "    agg_t = reshaped_frame.sum(axis=1)\n",
        "    agg_c = agg_t.sum(axis=-1)\n",
        "    return agg_c[:, 0, :] # in our case, we drop the polarity channel\n",
        "  return agg\n",
        "\n",
        "\n",
        "def pad_frame(frame_size):\n",
        "  \"\"\"Pads the frame to the given size\"\"\"\n",
        "  def pad(frame):\n",
        "    \"\"\"Input of frame is T, W, C\"\"\"\n",
        "    new_frame = np.zeros((frame_size[0], frame_size[1], frame_size[2]), np.float32)\n",
        "\n",
        "    shapes = []\n",
        "    for axis in range(3): # axis\n",
        "        min_shape = min(frame_size[axis], frame.shape[axis])\n",
        "        shapes.append(min_shape)\n",
        "\n",
        "    new_frame[:shapes[0], :shapes[1], :shapes[2]] = frame[:shapes[0], :shapes[1], :shapes[2]]\n",
        "    return new_frame\n",
        "  return pad\n",
        "\n",
        "\n",
        "# Create a composed transform that is applied to the events of the dataset\n",
        "composed_transforms = tonic.transforms.Compose([\n",
        "    crop_time_transform,\n",
        "    to_frame,\n",
        "    pad_frame((700,1,700)),\n",
        "    bin_frame(10, 5),\n",
        "])\n",
        "\n",
        "dataset_train = tonic.datasets.SHD(save_to= \"./data_shd\", train= True, transform=composed_transforms)\n",
        "dataset_test = tonic.datasets.SHD(save_to= \"./data_shd\", train= False, transform=composed_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mAz6Uw_NqBD"
      },
      "outputs": [],
      "source": [
        "events, label = dataset_train[0]\n",
        "events.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0QFgLyEGXJh"
      },
      "outputs": [],
      "source": [
        "# Transform to frame\n",
        "\n",
        "events, label = dataset_train[0]\n",
        "\n",
        "fig, axes = plt.subplots(1,1, sharey=True)\n",
        "axes = [axes]\n",
        "pos = axes[0].imshow(events[:,::-1].T, cmap=\"Blues\")\n",
        "axes[0].set_title(\"Label: {}\".format(label))\n",
        "axes[0].set_ylabel(\"Neuron idx (channel)\")\n",
        "axes[0].set_xlabel(\"Timestep (frame idx)\")\n",
        "fig.colorbar(pos, ax=axes[0], label=\"Spike count\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have sequences of 70 frames that can be fed as currents to the network."
      ],
      "metadata": {
        "id": "4mO1SJRx2NxW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaXVW1xmWVmi"
      },
      "source": [
        "# Part 3: Audio Classification using event-based data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tonic also provides a way to cache the dataset on the disk for faster loading during the training time."
      ],
      "metadata": {
        "id": "7YrKZtJh2SOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Efficient data loading"
      ],
      "metadata": {
        "id": "V_wWFxUUTolW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZFqNasUXRI6"
      },
      "outputs": [],
      "source": [
        "# train dataset caching\n",
        "train_audio_dataloader = DataLoader(\n",
        "    dataset_train,\n",
        "    shuffle=True,\n",
        "    batch_size=128,\n",
        "    collate_fn=tonic.collation.PadTensors(batch_first=True),\n",
        ")\n",
        "\n",
        "cached_dataset = tonic.DiskCachedDataset(dataset_train, cache_path=\"./cache/fast_dataloading_train\")\n",
        "train_audio_dataloader = DataLoader(cached_dataset, batch_size=128, num_workers=2, drop_last=True)\n",
        "\n",
        "# test dataset caching\n",
        "test_audio_dataloader = DataLoader(\n",
        "    dataset_test,\n",
        "    shuffle=True,\n",
        "    batch_size=128,\n",
        "    collate_fn=tonic.collation.PadTensors(batch_first=True),\n",
        ")\n",
        "\n",
        "cached_dataset = tonic.DiskCachedDataset(dataset_test, cache_path=\"./cache/fast_dataloading_test\")\n",
        "test_audio_dataloader = DataLoader(cached_dataset, batch_size=128, num_workers=2, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed forward Neural Network"
      ],
      "metadata": {
        "id": "ujIAt44MTxMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now define our first network, which will consist in IAF neurons with a surrogate function consisting of the derivative of the exponential function."
      ],
      "metadata": {
        "id": "JsfH5VHk2ZlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's first define three models using Sinabs\n",
        "from torch import nn\n",
        "import sinabs.layers as sl\n",
        "\n",
        "spike_fn = sinabs.activation.SingleSpike\n",
        "surr_fn = sinabs.activation.SingleExponential(grad_width=0.5, grad_scale=1.0)\n",
        "\n",
        "model_iaf= nn.Sequential(\n",
        "      nn.Linear(140, 64, bias=False), # weights (synapses) input-> 64 neurons\n",
        "      sl.IAF(min_v_mem=0, tau_syn=1, spike_fn=spike_fn, surrogate_grad_fn=surr_fn),\n",
        "      nn.Linear(64, 64, bias=False),\n",
        "      sl.IAF(min_v_mem=0, tau_syn=1, spike_fn=spike_fn, surrogate_grad_fn=surr_fn),\n",
        "      nn.Linear(64, 20, bias=False),\n",
        "      sl.IAF(min_v_mem=0, tau_syn=1, spike_fn=spike_fn, surrogate_grad_fn=surr_fn),\n",
        "  )"
      ],
      "metadata": {
        "id": "3ZS883jGh7NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, targets = next(iter(train_audio_dataloader))\n",
        "pred = model_iaf(data)\n",
        "\n",
        "print(\"Shapes: input {} - output {}\".format(data.shape, pred.shape))"
      ],
      "metadata": {
        "id": "mfgIighhiHg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sinabs also provides a faster implementation of the model, by parallelizing the timesteps. This \"flattens\" the time dimension, leading to vectors of lenghts $Batch size \\times Timesteps$. At the end, the output is transformed again to the original format (B, T, output dimension)."
      ],
      "metadata": {
        "id": "bAsbuRLB2ovU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxm_zoIBWbsI"
      },
      "outputs": [],
      "source": [
        "# Let's first define three models using Sinabs\n",
        "from torch import nn\n",
        "import sinabs.layers as sl\n",
        "\n",
        "optimized = True\n",
        "# for optimized version, use this:\n",
        "if optimized:\n",
        "  model_iaf= nn.Sequential(\n",
        "      sl.FlattenTime(), # for parallelization on GPU\n",
        "      nn.Linear(140, 64, bias=False),\n",
        "      sl.IAFSqueeze(batch_size=128, min_v_mem=0, tau_syn=1, spike_fn=spike_fn, surrogate_grad_fn=surr_fn),\n",
        "      nn.Linear(64, 64, bias=False),\n",
        "      sl.IAFSqueeze(batch_size=128, min_v_mem=0, tau_syn=1, spike_fn=spike_fn, surrogate_grad_fn=surr_fn),\n",
        "      nn.Linear(64, 20, bias=False),\n",
        "      sl.IAFSqueeze(batch_size=128, min_v_mem=0, tau_syn=1, spike_fn=spike_fn, surrogate_grad_fn=surr_fn),\n",
        "      sl.UnflattenTime(batch_size=128),\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSYe9nEtXeQP"
      },
      "outputs": [],
      "source": [
        "data_input, targets = next(iter(train_audio_dataloader))\n",
        "pred = model_iaf(data_input)\n",
        "\n",
        "print(\"Shapes: input {} - output {}\".format(data_input.shape, pred.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, Sinabs provides a few helper functions to estimate the computational cost (Synaptic operations - SynOps) of the model."
      ],
      "metadata": {
        "id": "OCGXFiHU3WUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(model, data):\n",
        "  analyzer = sinabs.synopcounter.SNNAnalyzer(model)\n",
        "  output = model(data)  # forward pass\n",
        "  model_stats = analyzer.get_model_statistics()\n",
        "  return model_stats"
      ],
      "metadata": {
        "id": "j0yO8SdqVJmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_stats_iaf_before = get_stats(model_iaf, data_input)\n",
        "model_stats_iaf_before"
      ],
      "metadata": {
        "id": "vvlV1j_hYF5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Training"
      ],
      "metadata": {
        "id": "aCjfwNnNT32D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that everything is setup, let's train the model for a few epochs using backpropagation."
      ],
      "metadata": {
        "id": "5B7Zq36B3YGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zqd5HkiqZvt7"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def train(model, dataloader, n_epochs, optimizer, crit):\n",
        "  model.train()\n",
        "  if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "  for epoch in range(n_epochs):\n",
        "      losses = []\n",
        "      for data, targets in tqdm(train_audio_dataloader):\n",
        "\n",
        "        # check if cuda\n",
        "        if torch.cuda.is_available():\n",
        "          data, targets = data.cuda(), targets.cuda()\n",
        "\n",
        "        sinabs.reset_states(model)  # each synapse and neuron membranes are reset to zero\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(data)\n",
        "        pred = y_hat.sum(1)\n",
        "        loss = crit(pred, targets)\n",
        "        loss.backward()\n",
        "        losses.append(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "      print(f\"Loss: {torch.stack(losses).mean()}\")\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 3\n",
        "optimizer = torch.optim.Adam(model_iaf.parameters(), lr=1e-3)\n",
        "crit = nn.functional.cross_entropy\n",
        "\n",
        "train(model_iaf, train_audio_dataloader, n_epochs, optimizer, crit)"
      ],
      "metadata": {
        "id": "sWhmpjPrc9Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and comparison"
      ],
      "metadata": {
        "id": "Fhk7NWKmT-7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the network is trained, let's evaluated it on both the training set and the test set."
      ],
      "metadata": {
        "id": "Ir262cgH3dlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate model accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def eval_model(model, dataloader):\n",
        "  model.eval()\n",
        "  if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "  all_targets = []\n",
        "  all_preds = []\n",
        "  with torch.no_grad():\n",
        "    test_losses = []\n",
        "    for data, targets in tqdm(dataloader):\n",
        "      if torch.cuda.is_available():\n",
        "        data, targets = data.cuda(), targets.cuda()\n",
        "\n",
        "      sinabs.reset_states(model)  # each synapse and neuron membranes are reset to zero\n",
        "      y_hat = model(data)\n",
        "      pred = y_hat.sum(1)\n",
        "\n",
        "      loss = crit(pred, targets)\n",
        "      test_losses.append(loss)\n",
        "\n",
        "      all_targets.append(targets.cpu().numpy())\n",
        "      all_preds.append(pred.cpu().numpy())\n",
        "\n",
        "  # print the accuracy\n",
        "  all_targets = np.concatenate(all_targets)\n",
        "  all_preds = np.concatenate(all_preds)\n",
        "\n",
        "  total_loss = torch.stack(test_losses).mean()\n",
        "  acc = accuracy_score(all_targets, np.argmax(all_preds, axis=1))\n",
        "  return acc, total_loss"
      ],
      "metadata": {
        "id": "uSvQzlaVUe37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train_iaf, loss_train = eval_model(model_iaf, train_audio_dataloader)\n",
        "acc_test_iaf, loss_test = eval_model(model_iaf, test_audio_dataloader)\n",
        "\n",
        "print(\"Train accuracy: {:.2f}%\".format(acc_train_iaf*100))\n",
        "print(\"Test accuracy: {:.2f}%\".format(acc_test_iaf*100))"
      ],
      "metadata": {
        "id": "1gs6j1Xcb3bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad, given the few epochs."
      ],
      "metadata": {
        "id": "hnrGCTSW3iC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_stats_iaf = get_stats(model_iaf, data_input)\n",
        "for k, v in model_stats_iaf.items():\n",
        "  print(\"{}: before = {:.2f} / after = {:.2f}\".format(k, v, model_stats_iaf_before[k]))"
      ],
      "metadata": {
        "id": "j6p4gSV-qDl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the firing rate did not change much. Consequently, the number of synaptic operations stayed stable.\n",
        "Therefore, the spiking patterns (spatio-temporal distribution) transmit the information."
      ],
      "metadata": {
        "id": "GLNtHACH3mO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison of computational cost of Spiking Models"
      ],
      "metadata": {
        "id": "BhQHK3CjUGQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also compare with the other neuron models: LIF and ALIF."
      ],
      "metadata": {
        "id": "Bk2SoY2j3-uV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LIF Model\n",
        "\n",
        "model_lif = nn.Sequential(\n",
        "    sl.FlattenTime(),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(140, 64, bias=False),\n",
        "    sl.LIFSqueeze(batch_size=128, min_v_mem=0, tau_syn=1, tau_mem=2),\n",
        "    nn.Linear(64, 64, bias=False),\n",
        "    sl.LIFSqueeze(batch_size=128, min_v_mem=0, tau_syn=1, tau_mem=2),\n",
        "    nn.Linear(64, 20, bias=False),\n",
        "    sl.LIFSqueeze(batch_size=128, min_v_mem=0, tau_syn=1, tau_mem=2),\n",
        "    sl.UnflattenTime(batch_size=128)\n",
        ")"
      ],
      "metadata": {
        "id": "XgiJpE6JUgTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_lif, train_audio_dataloader, n_epochs, optimizer, crit)\n",
        "print(\"Test...\")\n",
        "acc_train_lif, loss_train = eval_model(model_lif, train_audio_dataloader)\n",
        "acc_test_lif, loss_test = eval_model(model_lif, test_audio_dataloader)\n",
        "\n",
        "print(\"Train accuracy: {:.2f}%\".format(acc_train_lif*100))\n",
        "print(\"Test accuracy: {:.2f}%\".format(acc_test_lif*100))"
      ],
      "metadata": {
        "id": "raixqOjDen05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_alif = nn.Sequential(\n",
        "    nn.Linear(140, 64, bias=False),\n",
        "    sl.ALIF(min_v_mem=0., tau_syn=1., tau_mem=2., tau_adapt=1.0),\n",
        "    nn.Linear(64, 64, bias=False),\n",
        "    sl.ALIF(min_v_mem=0., tau_syn=1., tau_mem=2., tau_adapt=1.0),\n",
        "    nn.Linear(64, 20, bias=False),\n",
        "    sl.ALIF(min_v_mem=0., tau_syn=1., tau_mem=2., tau_adapt=1.0),\n",
        ")"
      ],
      "metadata": {
        "id": "GFa5f_BcekJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model_alif, train_audio_dataloader, n_epochs, optimizer, crit)\n",
        "\n",
        "print(\"Test...\")\n",
        "acc_train_alif, loss_train = eval_model(model_alif, train_audio_dataloader)\n",
        "acc_test_alif, loss_test = eval_model(model_alif, test_audio_dataloader)\n",
        "\n",
        "print(\"Train accuracy: {:.2f}%\".format(acc_train_alif*100))\n",
        "print(\"Test accuracy: {:.2f}%\".format(acc_test_alif*100))"
      ],
      "metadata": {
        "id": "yTpeT3L-e6UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... and plot their computational cost and accuracy."
      ],
      "metadata": {
        "id": "M-SbyHKZ4D_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of spiking models\n",
        "def get_stats(model, data):\n",
        "  analyzer = sinabs.synopcounter.SNNAnalyzer(model)\n",
        "  output = model(data)  # forward pass\n",
        "  model_stats = analyzer.get_model_statistics()\n",
        "  return model_stats\n",
        "\n",
        "data_input, labels = next(iter(train_audio_dataloader))\n",
        "\n",
        "stats_iaf = get_stats(model_iaf, data_input)\n",
        "stats_lif = get_stats(model_lif, data_input)\n",
        "stats_alif = get_stats(model_alif, data_input)"
      ],
      "metadata": {
        "id": "Ifu-lFHre7Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot SynOps vs Accuracy and Firing rate\n",
        "\n",
        "fig, axes = plt.subplots(1,2, sharey=True)\n",
        "axes[0].scatter(stats_lif[\"synops\"].detach().numpy(), acc_test_lif, label=\"LIF\")\n",
        "axes[0].scatter(stats_iaf[\"synops\"].detach().numpy(), acc_test_iaf, label=\"IAF\")\n",
        "axes[0].scatter(stats_alif[\"synops\"].detach().numpy(), acc_test_alif, label=\"ALIF\")\n",
        "axes[0].set_xlabel(\"SynOps\")\n",
        "axes[0].set_ylabel(\"Accuracy\")\n",
        "axes[0].set_title(\"Accuracy vs SynOps\")\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].scatter(stats_lif[\"firing_rate\"].detach().numpy(), acc_test_lif, label=\"LIF\")\n",
        "axes[1].scatter(stats_iaf[\"firing_rate\"].detach().numpy(), acc_test_iaf, label=\"IAF\")\n",
        "axes[1].scatter(stats_alif[\"firing_rate\"].detach().numpy(), acc_test_alif, label=\"ALIF\")\n",
        "axes[1].set_xlabel(\"Firing rate\")\n",
        "axes[1].set_ylabel(\"Accuracy\")\n",
        "axes[1].set_title(\"Accuracy vs Firing rate\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t9YDingAfuHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "We have successfully trained different spiking models on an audio spiking dataset using backpropagation. Each model have different properties. Neuromorphic engineers should carefully tune the hyperparameters of the neurons to balance efficiency and accuracy, and select the network model suited for the hardware at hand.\n",
        "\n",
        "While synaptic operations (SynOps) give insights about the model complexity, they often hide important computational cost. Alternative metrics are currently investigated by researchers."
      ],
      "metadata": {
        "id": "M9KI8Qhc4Hl0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxP2fNF4ePiH"
      },
      "source": [
        "# Bonus - Part 4: Image processing - the DVSGestures event-based dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OnJGSHxgV--"
      },
      "source": [
        "### Dataset loading and exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR9j3f7_eUu9"
      },
      "outputs": [],
      "source": [
        "dataset_train = tonic.datasets.DVSGesture(save_to= \"./data\", train= True)\n",
        "dataset_test = tonic.datasets.DVSGesture(save_to= \"./data\", train= False)\n",
        "\n",
        "print(\"Training samples:\", len(dataset_train))\n",
        "print(\"Testing samples:\", len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Kcter0KfD5F"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"matplotlib\").setLevel(logging.ERROR)  # Suppress matplotlib warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)  # Ignores UserWarnings\n",
        "\n",
        "original_events, label = dataset_train[0]\n",
        "\n",
        "transform = tonic.transforms.ToFrame(\n",
        "    sensor_size=tonic.datasets.DVSGesture.sensor_size,\n",
        "    time_window=10000,\n",
        ")\n",
        "\n",
        "frames = transform(original_events)\n",
        "animation = tonic.utils.plot_animation(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vav59QxkweZ-"
      },
      "outputs": [],
      "source": [
        "# Display the animation inline in a Jupyter notebook\n",
        "from IPython.display import HTML\n",
        "HTML(animation.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfJ9bDg0gYoR"
      },
      "outputs": [],
      "source": [
        "# Plot distribution of events\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "max_t = 0\n",
        "all_train_t = []\n",
        "for i in tqdm(range(len(dataset_train))):\n",
        "  t = dataset_train[i][0][\"t\"].tolist()\n",
        "  max_t = max(max_t, max(t))\n",
        "  t = t[::10]  # take only every 10 events, because of limited memory in notebook\n",
        "  all_train_t+=t\n",
        "\n",
        "all_test_t = []\n",
        "for i in tqdm(range(len(dataset_test))):\n",
        "  t = dataset_test[i][0][\"t\"].tolist()\n",
        "  max_t = max(max_t, max(t))\n",
        "  t = t[::10]  # take only every 10 events, because of limited memory in notebook\n",
        "  all_test_t+=t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jAbqEAEp3zA"
      },
      "outputs": [],
      "source": [
        "hist_train, bin_edges_train = np.histogram(all_train_t, bins=100)\n",
        "hist_test, bin_edges_test = np.histogram(all_test_t, bins=100)\n",
        "# Compute the 95th percentile\n",
        "percentile = 99\n",
        "p_train = np.percentile(all_train_t, percentile)\n",
        "p_test = np.percentile(all_test_t, percentile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XeX4zUOqISG"
      },
      "outputs": [],
      "source": [
        "# Compute bin centers\n",
        "bin_centers_train = (bin_edges_train[:-1] + bin_edges_train[1:]) / 2\n",
        "bin_centers_test = (bin_edges_test[:-1] + bin_edges_test[1:]) / 2\n",
        "\n",
        "# Plot histogram\n",
        "fig, ax = plt.subplots(1,1)\n",
        "axes = [ax]\n",
        "# histogram\n",
        "axes[0].bar(bin_centers_train, hist_train, width=np.diff(bin_edges_train), edgecolor=None, alpha=0.7, label=\"train\")\n",
        "axes[0].bar(bin_centers_test, hist_test, width=np.diff(bin_edges_test), edgecolor=None, alpha=0.7, label=\"test\")\n",
        "axes[0].set_xlabel(\"Time (us)\")\n",
        "axes[0].set_ylabel(\"Frequency\")\n",
        "axes[0].set_title(\"Histogram\")\n",
        "\n",
        "# plot percentile vline\n",
        "axes[0].axvline(p_train, color=colors[0], linestyle='--', label=\"{}th percentile (train)\".format(percentile))\n",
        "axes[0].axvline(p_test, color=colors[1], linestyle='--', label=\"{}th percentile (test)\".format(percentile))\n",
        "# plot max_t\n",
        "axes[0].axvline(max_t, color=colors[2], linestyle='--', label=r'$t_{max}$')\n",
        "\n",
        "axes[0].legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61Tm4BIIw_Mo"
      },
      "source": [
        "### Dataset preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju1iE1oLx339"
      },
      "outputs": [],
      "source": [
        "sensor_size = tonic.datasets.DVSGesture.sensor_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7S-MYriijS-"
      },
      "outputs": [],
      "source": [
        "# As shown above, 99% of the events happen before 10s, while the maximum duration of a frame is about 18sec.\n",
        "# Therefore, we will first crop each sequence before 10sec, and then split every sample in 1sec.\n",
        "# This way, we will classify 1sec samples.\n",
        "# For the purpose of this tutorial, we will also downsample the images by 4, leading to 64x64x2 samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBr5IQ3TyDjT"
      },
      "outputs": [],
      "source": [
        "# create transforms\n",
        "sensor_size = tonic.datasets.DVSGesture.sensor_size # 128x128x2\n",
        "downsample = 2\n",
        "new_sensor_size = (sensor_size[0] // downsample, sensor_size[1] // downsample, 2)\n",
        "\n",
        "crop_time_transform = tonic.transforms.CropTime(max=10e6)\n",
        "downsample_transform = tonic.transforms.Downsample(spatial_factor=1.0/downsample)\n",
        "\n",
        "# Create a composed transform that is applied to the events of the dataset\n",
        "composed_transforms = tonic.transforms.Compose([\n",
        "    crop_time_transform,\n",
        "    downsample_transform\n",
        "])\n",
        "\n",
        "dataset_train = tonic.datasets.DVSGesture(save_to= \"./data\", train= True, transform=composed_transforms)\n",
        "dataset_test = tonic.datasets.DVSGesture(save_to= \"./data\", train= False, transform=composed_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9qvazgE0SGI"
      },
      "outputs": [],
      "source": [
        "# plot a new sample\n",
        "transformed_events, label = dataset_train[0]\n",
        "\n",
        "transform = tonic.transforms.ToFrame(\n",
        "    sensor_size=new_sensor_size,\n",
        "    time_window=10e3,\n",
        ")\n",
        "\n",
        "frames_transformed = transform(transformed_events)\n",
        "animation_transformed = tonic.utils.plot_animation(frames_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUpxq5UG2KB5"
      },
      "outputs": [],
      "source": [
        "transformed_events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYXZSyrI17Q3"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "HTML(animation_transformed.to_jshtml())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDlPV29qz8hW"
      },
      "outputs": [],
      "source": [
        "from tonic import SlicedDataset\n",
        "\n",
        "frame_transform = tonic.transforms.ToFrame(\n",
        "    sensor_size=new_sensor_size,\n",
        "    time_window=1e5,\n",
        ")\n",
        "\n",
        "slicing_time_window = 1e6 # microseconds\n",
        "slicer = tonic.slicers.SliceByTime(time_window=slicing_time_window)\n",
        "\n",
        "# slice the train dataset\n",
        "sliced_dataset_train = SlicedDataset(\n",
        "    dataset_train, slicer=slicer, metadata_path=\"./metadata/dvs_train\",\n",
        "    transform=frame_transform,\n",
        ")\n",
        "\n",
        "# slice the test dataset\n",
        "sliced_dataset_test = SlicedDataset(\n",
        "    dataset_test, slicer=slicer, metadata_path=\"./metadata/dvs_test\",\n",
        "    transform=frame_transform,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmNRWIfV4CNq"
      },
      "outputs": [],
      "source": [
        "print(\"Sliced datasets now lead to:\")\n",
        "print(\"Training: {} vs {}\".format(len(dataset_train), len(sliced_dataset_train)))\n",
        "print(\"Testing: {} vs {}\".format(len(dataset_test), len(sliced_dataset_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HkogUgj4fqz"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "# make sure every sequence has the same size\n",
        "trainloader = DataLoader(\n",
        "    sliced_dataset_train,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=tonic.collation.PadTensors(batch_first=True),\n",
        ")\n",
        "\n",
        "# make sure every sequence has the same size\n",
        "testloader = DataLoader(\n",
        "    sliced_dataset_train,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=tonic.collation.PadTensors(batch_first=True),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sULDMpCt4z3V"
      },
      "outputs": [],
      "source": [
        "frames, targets = next(iter(trainloader))\n",
        "frames.shape, targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp4Uvcqe6_VX"
      },
      "source": [
        "### Sample batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8El3TEn4_CL"
      },
      "outputs": [],
      "source": [
        "# plot the batch\n",
        "\n",
        "fig, axes = plt.subplots(3,9)\n",
        "fig.set_figwidth(15)\n",
        "fig.set_figheight(5)\n",
        "\n",
        "# create empty RGB frame holder\n",
        "frame_holder = np.zeros((batch_size,9,3,new_sensor_size[0],new_sensor_size[1]))\n",
        "frame_holder[:,:, 1, :,:] = frames[:,:, 0, :,:]\n",
        "frame_holder[:,:, 2, :,:] = frames[:,:, 1, :,:]\n",
        "\n",
        "\n",
        "for b_idx in range(3):\n",
        "    for f_idx in range(frames.shape[1]):\n",
        "        ax = axes[b_idx, f_idx]\n",
        "        frame_to_show = frame_holder[b_idx, f_idx, :,:,:]\n",
        "        ax.imshow(np.moveaxis(frame_to_show, 0, 2))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeJs3Foybeuf"
      },
      "source": [
        "# Bonus - Part 5: Training a Spiking Neural Network for Vision Applications"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaVlU1idZp-f"
      },
      "outputs": [],
      "source": [
        "import sinabs.layers as sl\n",
        "import sinabs.exodus.layers as sel\n",
        "from torch import nn\n",
        "\n",
        "backend = sl # Sinabs\n",
        "backend = sel # Sinabs EXODUS\n",
        "\n",
        "model_iaf = nn.Sequential(\n",
        "    sl.FlattenTime(),\n",
        "    nn.Conv2d(2, 8, kernel_size=5, padding=1, bias=False),\n",
        "    backend.IAFSqueeze(batch_size=batch_size, min_v_mem=-1),\n",
        "    sl.SumPool2d(2),\n",
        "    nn.Conv2d(8, 16, kernel_size=3, padding=1, bias=False),\n",
        "    backend.IAFSqueeze(batch_size=batch_size, min_v_mem=-1),\n",
        "    sl.SumPool2d(2),\n",
        "    nn.Conv2d(16, 32, kernel_size=3, padding=1, bias=False),\n",
        "    backend.IAFSqueeze(batch_size=batch_size, min_v_mem=-1),\n",
        "    sl.SumPool2d(2),\n",
        "    nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
        "    backend.IAFSqueeze(batch_size=batch_size, min_v_mem=-1),\n",
        "    sl.SumPool2d(2),\n",
        "    nn.Conv2d(64, 11, kernel_size=3, padding=0, bias=False),\n",
        "    backend.IAFSqueeze(batch_size=batch_size, min_v_mem=-1),\n",
        "    nn.Flatten(),\n",
        "    sl.UnflattenTime(batch_size=batch_size),\n",
        ").cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOu4JBWgBQrH"
      },
      "outputs": [],
      "source": [
        "y_hat = model_iaf(frames.cuda())\n",
        "y_hat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vlKE1w_b2fs"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "n_epochs = 1\n",
        "optimizer = torch.optim.Adam(model_iaf.parameters(), lr=1e-3)\n",
        "crit = nn.functional.cross_entropy\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    losses = []\n",
        "    for data, targets in tqdm(trainloader):\n",
        "        data, targets = data.cuda(), targets.cuda()\n",
        "        sinabs.reset_states(model_iaf)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model_iaf(data)\n",
        "        pred = y_hat.sum(1)\n",
        "        loss = crit(pred, targets)\n",
        "        loss.backward()\n",
        "        losses.append(loss)\n",
        "        optimizer.step()\n",
        "    print(f\"Loss: {torch.stack(losses).mean()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "collapsed_sections": [
        "HXz7myh-yCoV",
        "CbIts868H_iJ",
        "nVgLwIBbEZ9N",
        "jaXVW1xmWVmi",
        "WxP2fNF4ePiH",
        "WeJs3Foybeuf"
      ],
      "authorship_tag": "ABX9TyMGjaIhumnM/C+2CQ7UI906",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}